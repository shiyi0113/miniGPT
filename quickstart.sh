#!/bin/bash
set -e  # é‡åˆ°ä»»ä½•é”™è¯¯ç«‹å³åœæ­¢æ‰§è¡Œ

echo "ğŸš€ [MiniGPT] å¼€å§‹å…¨æµç¨‹å¤ç°..."

# ==========================================
# 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥ä¸å®‰è£… (Infra Layer)
# ==========================================
echo "ğŸ› ï¸ [1/5] æ£€æŸ¥åŸºç¡€ç¯å¢ƒ..."

# æ£€æŸ¥/å®‰è£… Rust
if ! command -v cargo &> /dev/null; then
    echo "   -> å®‰è£… Rust..."
    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
    source "$HOME/.cargo/env"
else
    echo "   -> Rust å·²å°±ç»ª"
fi

# æ£€æŸ¥/å®‰è£… uv
if ! command -v uv &> /dev/null; then
    echo "   -> å®‰è£… uv..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
    source "$HOME/.cargo/env" 2>/dev/null || true
else
    echo "   -> uv å·²å°±ç»ª"
fi

# æ£€æŸ¥ Python å¼€å‘å¤´æ–‡ä»¶ (é’ˆå¯¹ Linux ç¼–è¯‘ PyTorch/Triton)
# æ³¨æ„ï¼šè¿™æ­¥é€šå¸¸éœ€è¦ sudoï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰ sudo æƒé™å¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†
if [ -f /etc/debian_version ]; then
    echo "   -> å°è¯•å®‰è£… python3.12-dev (éœ€è¦ sudo å¯†ç )..."
    sudo apt-get update && sudo apt-get install -y python3.12-dev || echo "âš ï¸ è‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²å®‰è£… python3.12-dev"
fi

# ==========================================
# 2. é¡¹ç›®ç¼–è¯‘ä¸ä¾èµ– (Compilation Layer)
# ==========================================
echo "âš™ï¸ [2/5] æ„å»ºé¡¹ç›®ç¯å¢ƒ..."

# åŒæ­¥ Python ä¾èµ–
uv sync

# ç¼–è¯‘ Rust æ‰©å±• (rustbpe)
# ä½¿ç”¨ --release æ¨¡å¼ç¼–è¯‘ä»¥è·å¾—æœ€é«˜æ€§èƒ½
echo "   -> ç¼–è¯‘ rustbpe æ‰©å±•..."
uv run maturin develop --release

# ==========================================
# 3. æ•°æ®æµæ°´çº¿ (Data Pipeline)
# ==========================================
echo "ğŸŒŠ [3/5] å‡†å¤‡æ•°æ®..."

# 3.1 ä¸‹è½½æ•°æ® (é»˜è®¤ä¸‹è½½å‰ 5 ä¸ªåˆ†ç‰‡ç”¨äºæµ‹è¯•ï¼Œæƒ³è·‘å…¨é‡è¯·æ”¹ä¸º -n -1)
echo "   -> ä¸‹è½½ FineWeb-Edu æ•°æ®é›† (å‰5ä¸ªåˆ†ç‰‡)..."
uv run -m minigpt.dataset -n 5

# 3.2 è®­ç»ƒ Tokenizer
echo "   -> è®­ç»ƒ Tokenizer (ç”Ÿæˆ tokenizer.pkl)..."
# è¿™ä¼šè¯»å–ä¸‹è½½çš„æ•°æ®ï¼Œè®­ç»ƒè¯è¡¨ï¼Œå¹¶ä¿å­˜åˆ° ~/.cache/minigpt/tokenizer
uv run -m tests.tok_train

# ==========================================
# 4. æ¨¡å‹è®­ç»ƒ (Training Loop)
# ==========================================
echo "ğŸ”¥ [4/5] å¼€å§‹è®­ç»ƒæ¨¡å‹..."

# æ¸…ç†æ—§çš„è¾“å‡ºç›®å½• (å¯é€‰)
rm -rf output

# å¯åŠ¨è®­ç»ƒ
# å‚æ•°è¯´æ˜ï¼š
# --batch_size 4: é€‚é… 8GB æ˜¾å­˜
# --sequence_len 512: å‡å°ä¸Šä¸‹æ–‡é•¿åº¦ä»¥èŠ‚çœæ˜¾å­˜
# --total_steps 200: è·‘ 200 æ­¥ç”¨äºæ¼”ç¤º (ä½ å¯ä»¥æ”¹æˆ 5000 æˆ–æ›´å¤š)
# --save_every 200: è®­ç»ƒç»“æŸæ—¶ä¿å­˜ä¸€æ¬¡
uv run -m tests.train \
    --batch_size 4 \
    --sequence_len 512 \
    --total_steps 200 \
    --save_every 200 \
    --output_dir output

# ==========================================
# 5. æ¨ç†éªŒè¯ (Inference)
# ==========================================
echo "ğŸ¤– [5/5] å¯åŠ¨å¯¹è¯..."

CHECKPOINT="output/ckpt_00199.pt" # æ³¨æ„ï¼šè¿™é‡Œçš„æ­¥æ•°è¦å’Œ total_steps å¯¹åº”

if [ -f "$CHECKPOINT" ]; then
    echo "âœ… è®­ç»ƒå®Œæˆï¼æ­£åœ¨åŠ è½½æ¨¡å‹: $CHECKPOINT"
    echo "---------------------------------------------------"
    # å¯åŠ¨èŠå¤©è„šæœ¬
    uv run -m tests.chat --ckpt "$CHECKPOINT"
else
    echo "âŒ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œè®­ç»ƒå¯èƒ½å¤±è´¥ã€‚"
    exit 1
fi