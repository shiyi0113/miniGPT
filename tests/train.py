"""
MiniGPT è®­ç»ƒè„šæœ¬ (å•å¡/å¤šå¡é€šç”¨)
"""
import os
import time
import math
import argparse
from dataclasses import asdict

import torch
import torch.nn.functional as F

from minigpt.common import compute_init, compute_cleanup, print0
from minigpt.gpt import GPT, GPTConfig
from minigpt.dataloader import tokenizing_distributed_data_loader
from minigpt.report import get_report

# -----------------------------------------------------------------------------
# å­¦ä¹ ç‡è°ƒåº¦å™¨ (Cosine Decay with Warmup)
def get_lr(it, total_iters, warmup_iters, max_lr, min_lr):
    # 1. é¢„çƒ­é˜¶æ®µ (Linear Warmup)
    if it < warmup_iters:
        return max_lr * (it + 1) / warmup_iters
    # 2. è®­ç»ƒç»“æŸ (Min LR)
    if it > total_iters:
        return min_lr
    # 3. ä½™å¼¦è¡°å‡ (Cosine Decay)
    decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (max_lr - min_lr)

# -----------------------------------------------------------------------------
# ä¸»è®­ç»ƒé€»è¾‘
def main():
    parser = argparse.ArgumentParser()
    # æ¨¡å‹é…ç½®
    parser.add_argument("--n_layer", type=int, default=12, help="å±‚æ•°")
    parser.add_argument("--n_head", type=int, default=6, help="å¤´æ•°")
    parser.add_argument("--n_embd", type=int, default=768, help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--sequence_len", type=int, default=1024, help="ä¸Šä¸‹æ–‡é•¿åº¦")
    # è®­ç»ƒé…ç½®
    parser.add_argument("--batch_size", type=int, default=8, help="Micro Batch Size (æ¯å¼ å¡çš„æ‰¹æ¬¡å¤§å°)")
    parser.add_argument("--total_steps", type=int, default=1000, help="æ€»è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--warmup_steps", type=int, default=100, help="é¢„çƒ­æ­¥æ•°")
    parser.add_argument("--learning_rate", type=float, default=6e-4, help="æœ€å¤§å­¦ä¹ ç‡")
    parser.add_argument("--output_dir", type=str, default="output", help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    # è¿è¡Œé…ç½®
    parser.add_argument("--val_every", type=int, default=200, help="æ¯éš”å¤šå°‘æ­¥éªŒè¯ä¸€æ¬¡")
    parser.add_argument("--save_every", type=int, default=500, help="æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡")
    args = parser.parse_args()

    # 1. ç¯å¢ƒåˆå§‹åŒ–
    ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init()
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    print0(f"Initializing GPT model...")
    config = GPTConfig(
        n_layer=args.n_layer, 
        n_head=args.n_head, 
        n_embd=args.n_embd, 
        sequence_len=args.sequence_len,
        vocab_size=50304 # ä¸ Tokenizer è®­ç»ƒæ—¶ä¸€è‡´
    )
    model = GPT(config)
    model.init_weights()
    model.to(device)
    
    # DDP åŒ…è£…
    if ddp:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[ddp_local_rank])
        raw_model = model.module
    else:
        raw_model = model

    # 3. ä¼˜åŒ–å™¨
    optimizers = raw_model.setup_optimizers(
        weight_decay=0.1, 
        unembedding_lr=10 * args.learning_rate, # æœ€åä¸€å±‚é€šå¸¸ç”¨æ›´å¤§çš„å­¦ä¹ ç‡
        embedding_lr=10 * args.learning_rate, 
        matrix_lr=args.learning_rate
    )

    # 4. æ•°æ®åŠ è½½å™¨
    # æ³¨æ„ï¼štokenizer_batch_size æ˜¯ CPU ç«¯å¹¶è¡Œå¤„ç†çš„å¤§å°
    train_loader = tokenizing_distributed_data_loader(
        args.batch_size, args.sequence_len, split="train", device="cuda" if torch.cuda.is_available() else "cpu"
    )
    val_loader = tokenizing_distributed_data_loader(
        args.batch_size, args.sequence_len, split="val", device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # 5. è®­ç»ƒå¾ªç¯
    print0("Starting training loop...")
    report = get_report()
    total_time = 0
    
    for step in range(args.total_steps):
        t0 = time.time()
        
        # --- å­¦ä¹ ç‡è°ƒåº¦ ---
        lr = get_lr(step, args.total_steps, args.warmup_steps, args.learning_rate, args.learning_rate * 0.1)
        for opt in optimizers:
            for param_group in opt.param_groups:
                param_group['lr'] = lr
        
        # --- å‰å‘ä¼ æ’­ & åå‘ä¼ æ’­ ---
        # è·å–ä¸€ä¸ª Batch
        inputs, targets, _ = next(train_loader)
        
        # å‰å‘è®¡ç®— Loss
        # ä½¿ç”¨ BFloat16 æ··åˆç²¾åº¦ (RTX 30/40/50 å¿…å¤‡)
        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):
            loss = model(inputs, targets)
            
        # åå‘ä¼ æ’­
        loss.backward()
        
        # --- æ¢¯åº¦æ›´æ–° ---
        # æ¢¯åº¦è£å‰ª (é˜²æ­¢çˆ†ç‚¸)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        # ä¼˜åŒ–å™¨æ­¥è¿›
        for opt in optimizers:
            opt.step()
            opt.zero_grad(set_to_none=True) # æ¸…ç©ºæ¢¯åº¦
            
        # ç­‰å¾… GPU å®Œæˆ (ç”¨äºè®¡æ—¶å‡†ç¡®)
        torch.cuda.synchronize()
        t1 = time.time()
        dt = t1 - t0
        total_time += dt
        
        # --- æ—¥å¿—æ‰“å° ---
        if step % 10 == 0:
            tokens_per_sec = (args.batch_size * args.sequence_len * ddp_world_size) / dt
            print0(f"step {step:4d}/{args.total_steps} | loss {loss.item():.4f} | lr {lr:.2e} | {dt*1000:.1f}ms | {tokens_per_sec:.0f} tok/s")
            
            report.log("Training", {
                "step": step, "loss": loss.item(), "lr": lr, "dt": dt
            })

        # --- éªŒè¯å¾ªç¯ (Validation) ---
        if step > 0 and (step % args.val_every == 0 or step == args.total_steps - 1):
            print0(f"Running validation...")
            model.eval()
            val_loss = 0
            val_steps = 20 # éªŒè¯ 20 ä¸ª batch
            with torch.no_grad():
                for _ in range(val_steps):
                    inputs_v, targets_v, _ = next(val_loader)
                    with torch.autocast(device_type=device.type, dtype=torch.bfloat16):
                        loss_v = model(inputs_v, targets_v)
                    val_loss += loss_v.item()
            val_loss /= val_steps
            print0(f"âœ… Validation loss: {val_loss:.4f}")
            report.log("Validation", {"step": step, "val_loss": val_loss})
            model.train() # åˆ‡å›è®­ç»ƒæ¨¡å¼

        # --- æ¨¡å‹ä¿å­˜ (Checkpoint) ---
        if step > 0 and (step % args.save_every == 0 or step == args.total_steps - 1):
            if ddp_rank == 0:
                ckpt_path = os.path.join(args.output_dir, f"ckpt_{step:05d}.pt")
                checkpoint = {
                    "model": raw_model.state_dict(),
                    "config": asdict(config),
                    "step": step,
                    "val_loss": val_loss if 'val_loss' in locals() else None
                }
                torch.save(checkpoint, ckpt_path)
                print0(f"ğŸ’¾ Saved checkpoint to {ckpt_path}")

    compute_cleanup()
    print0("Training finished!")

if __name__ == "__main__":
    main()