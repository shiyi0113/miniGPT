import torch
from minigpt.gpt import GPT, GPTConfig
from minigpt.common import autodetect_device_type

def test_gpt_smoke():
    # 1. é…ç½®ä¸€ä¸ªè¿·ä½ çš„ GPT ç”¨äºæµ‹è¯• (ä¸ºäº†é€Ÿåº¦å’Œæ˜¾å­˜ï¼Œå‚æ•°è®¾å¾—å¾ˆå°)
    config = GPTConfig(
        sequence_len=32,   # ä¸Šä¸‹æ–‡é•¿åº¦
        vocab_size=512,    # è¯è¡¨å¤§å°
        n_layer=2,         # å±‚æ•°
        n_head=4,          # Query å¤´æ•°
        n_kv_head=2,       # KV å¤´æ•° (æµ‹è¯• GQA)
        n_embd=64          # åµŒå…¥ç»´åº¦
    )
    
    # 2. è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ (RTX 5060 -> cuda)
    device_type = autodetect_device_type()
    device = torch.device(device_type)
    
    print(f"ğŸ“¦ åˆå§‹åŒ–æ¨¡å‹: {config}")
    try:
        model = GPT(config)
        model.to(device)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # 3. å‰å‘ä¼ æ’­æµ‹è¯• (Forward Pass)
    print("ğŸ”„ æµ‹è¯•å‰å‘ä¼ æ’­ (Forward Pass)...")
    batch_size = 2
    seq_len = 16
    # éšæœºç”Ÿæˆä¸€äº› token ID
    dummy_idx = torch.randint(0, config.vocab_size, (batch_size, seq_len), device=device)
    
    try:
        # æµ‹è¯• Logits è¾“å‡º
        logits = model(dummy_idx)
        expected_shape = (batch_size, seq_len, config.vocab_size)
        assert logits.shape == expected_shape, f"Logits å½¢çŠ¶é”™è¯¯: {logits.shape} != {expected_shape}"
        print(f"âœ… Logits è®¡ç®—æˆåŠŸï¼Œå½¢çŠ¶: {logits.shape}")
        
        # æµ‹è¯• Loss è®¡ç®—
        loss = model(dummy_idx, targets=dummy_idx) # è‡ªå›å½’ä»»åŠ¡ targets é€šå¸¸æ˜¯ idx ç§»ä½ï¼Œè¿™é‡Œä»…æµ‹è¯•èƒ½å¦è¿è¡Œ
        assert loss.ndim == 0, "Loss åº”è¯¥æ˜¯ä¸€ä¸ªæ ‡é‡"
        print(f"âœ… Loss è®¡ç®—æˆåŠŸ: {loss.item():.4f}")
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        # æ‰“å°è¯¦ç»†é”™è¯¯æ ˆä»¥ä¾¿è°ƒè¯•
        import traceback
        traceback.print_exc()
        return

    # 4. ç”Ÿæˆæµ‹è¯• (Generation)
    print("âœ¨ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ (Generation)...")
    try:
        start_tokens = [1, 2, 3] # å‡è®¾çš„èµ·å§‹ token
        # ç”Ÿæˆ 5 ä¸ªæ–° token
        gen_len = 5
        
        # generate åª yield æ–°ç”Ÿæˆçš„ token
        generated = list(model.generate(start_tokens, max_tokens=gen_len))
        
        # ä¿®æ­£æ–­è¨€ï¼šé•¿åº¦åº”è¯¥åªç­‰äºç”Ÿæˆçš„é•¿åº¦ (5)ï¼Œè€Œä¸æ˜¯ æ€»é•¿åº¦ (8)
        assert len(generated) == gen_len, f"ç”Ÿæˆé•¿åº¦ä¸ç¬¦åˆé¢„æœŸ: {len(generated)} != {gen_len}"
        
        # æ‰‹åŠ¨æ‹¼æ¥ä»¥ä¾¿æ‰“å°æŸ¥çœ‹
        full_sequence = start_tokens + generated
        print(f"âœ… ç”ŸæˆæˆåŠŸ: è¾“å…¥={start_tokens} -> æ–°ç”Ÿæˆ={generated}")
        print(f"   å®Œæ•´åºåˆ—: {full_sequence}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    print("\nğŸ‰ æ­å–œï¼GPT æ¨¡å‹æ¶æ„é€šè¿‡äº†æ‰€æœ‰å†’çƒŸæµ‹è¯•ï¼")

if __name__ == "__main__":
    test_gpt_smoke()