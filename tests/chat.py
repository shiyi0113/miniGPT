"""
MiniGPT äº¤äº’å¼èŠå¤©è„šæœ¬ (CLI)
"""
import os
import time
import torch
import argparse
from minigpt.gpt import GPT, GPTConfig
from minigpt.tokenizer import get_tokenizer
from minigpt.common import autodetect_device_type

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--ckpt", type=str, default="output/ckpt_00099.pt", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--temperature", type=float, default=0.8, help="ç”Ÿæˆæ¸©åº¦ (è¶Šé«˜è¶Šéšæœº)")
    parser.add_argument("--top_k", type=int, default=200, help="Top-K é‡‡æ ·")
    parser.add_argument("--max_new_tokens", type=int, default=200, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    args = parser.parse_args()

    # 1. å‡†å¤‡è®¾å¤‡
    device_type = autodetect_device_type()
    device = torch.device(device_type)
    print(f"âœ¨ Using device: {device}")

    # 2. åŠ è½½ Tokenizer
    print("ğŸ“š Loading tokenizer...")
    tokenizer = get_tokenizer()

    # 3. åŠ è½½æ¨¡å‹ Checkpoint
    if not os.path.exists(args.ckpt):
        print(f"âŒ Error: Checkpoint not found at {args.ckpt}")
        return

    print(f"ğŸ“¦ Loading model from {args.ckpt}...")
    checkpoint = torch.load(args.ckpt, map_location=device)
    
    # ä» checkpoint ä¸­æ¢å¤é…ç½®
    # è¿™ä¸€ç‚¹å¾ˆå…³é”®ï¼šå¿…é¡»ç”¨è®­ç»ƒæ—¶çš„åŒæ ·é…ç½®æ¥å®ä¾‹åŒ–æ¨¡å‹
    gpt_conf = GPTConfig(**checkpoint["config"])
    model = GPT(gpt_conf)
    
    # åŠ è½½æƒé‡
    state_dict = checkpoint["model"]
    # å¤„ç†å¯èƒ½çš„ DDP å‰ç¼€ (å¦‚æœæ˜¯ç”¨å¤šå¡è®­ç»ƒçš„ï¼Œkey å¯èƒ½ä¼šæœ‰ "_orig_mod." æˆ– "module.")
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
            
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å…³é—­ Dropout ç­‰)
    print("âœ… Model loaded successfully!")

    # 4. è¿›å…¥èŠå¤©å¾ªç¯
    print("\nğŸ’¬ å¼€å§‹å¯¹è¯ (è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º)")
    print("-" * 50)

    while True:
        try:
            prompt = input("User: ")
        except EOFError:
            break
            
        if prompt.lower() in ["exit", "quit"]:
            break
        
        if not prompt.strip():
            continue

        # ç¼–ç è¾“å…¥
        # è¿™é‡Œæˆ‘ä»¬è¦æ‰‹åŠ¨æ„é€ å¯¹è¯æ ¼å¼å—ï¼Ÿ
        # ä¸ºäº†ç®€å•æ¼”ç¤ºï¼Œæˆ‘ä»¬å…ˆç›´æ¥æŠŠç”¨æˆ·è¾“å…¥å½“æˆ prompt (ç»­å†™æ¨¡å¼)ï¼Œä¸åŠ ç‰¹æ®Šçš„ Chat æ¨¡æ¿
        # å¦‚æœæ¨¡å‹è¶³å¤Ÿå¼ºï¼Œå®ƒä¼šå­¦ä¼šç»­å†™ï¼›å¦‚æœæ¨¡å‹å¾ˆå¼±ï¼Œå®ƒä¼šä¹±è¯´ã€‚
        
        # ç®€å•çš„ Encode
        input_ids = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())
        
        # ç”Ÿæˆ
        print("Assistant: ", end="", flush=True)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        t0 = time.time()
        
        # ç”Ÿæˆå¾ªç¯
        gen_tokens = []
        # generate è¿”å›çš„æ˜¯ generatorï¼Œæˆ‘ä»¬å¯ä»¥è¿­ä»£å®ƒæ¥å®ç°æµå¼æ‰“å°
        # ä½†è¦æ³¨æ„æˆ‘ä»¬çš„ generate å®ç°ç›®å‰æ˜¯ yield æ¯ä¸€ä¸ª token
        for token_id in model.generate(input_ids, args.max_new_tokens, temperature=args.temperature, top_k=args.top_k):
            # è§£ç å½“å‰ token
            # æ³¨æ„ï¼šå•ä¸ª token è§£ç å¯èƒ½ä¼šä¹±ç ï¼ˆå¯¹äºå¤šå­—èŠ‚å­—ç¬¦ï¼‰ï¼Œä½†è¿™åœ¨è‹±æ–‡è¯­å¢ƒä¸‹é€šå¸¸æ²¡é—®é¢˜
            # ä¸¥è°¨åšæ³•æ˜¯ç§¯æ”’ bytes å†è§£ç ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            word = tokenizer.decode([token_id])
            print(word, end="", flush=True)
            gen_tokens.append(token_id)
            
        print("\n")
        t1 = time.time()
        tokens_sec = len(gen_tokens) / (t1 - t0)
        print(f"--- (Speed: {tokens_sec:.2f} tok/s) ---")
        print("-" * 50)

if __name__ == "__main__":
    main()